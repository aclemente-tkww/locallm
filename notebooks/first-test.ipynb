{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1749cee6-ff54-4397-9739-acc8b8c56da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T16:36:01.192719Z",
     "iopub.status.busy": "2023-11-22T16:36:01.191991Z",
     "iopub.status.idle": "2023-11-22T16:36:02.497352Z",
     "shell.execute_reply": "2023-11-22T16:36:02.497055Z",
     "shell.execute_reply.started": "2023-11-22T16:36:01.192684Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4597a5cb-3b33-4a33-8c4a-2569c0af165e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T16:48:08.569783Z",
     "iopub.status.busy": "2023-11-22T16:48:08.569255Z",
     "iopub.status.idle": "2023-11-22T16:48:08.577309Z",
     "shell.execute_reply": "2023-11-22T16:48:08.575640Z",
     "shell.execute_reply.started": "2023-11-22T16:48:08.569752Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = \"GeneZC/MiniChat-3B\"  # I could not get this to work with Apple\n",
    "model = \"vihangd/shearedplats-2.7b-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7473c7-4704-410d-9226-4968c3b50fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:09:30.455154Z",
     "iopub.status.busy": "2023-11-22T17:09:30.454246Z",
     "iopub.status.idle": "2023-11-22T17:09:55.785942Z",
     "shell.execute_reply": "2023-11-22T17:09:55.783626Z",
     "shell.execute_reply.started": "2023-11-22T17:09:30.455085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc64f0b5ed4e417783b0eb3164e72938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebf3d3dc-e4bd-4e84-9d9e-bfd241abd786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:29:33.649855Z",
     "iopub.status.busy": "2023-11-22T17:29:33.649591Z",
     "iopub.status.idle": "2023-11-22T18:08:38.596574Z",
     "shell.execute_reply": "2023-11-22T18:08:38.590277Z",
     "shell.execute_reply.started": "2023-11-22T17:29:33.649838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the answer to the universe, life and everything else?\\n\\n### [Question 2: What is the answer to the question \"What is the answer to the universe, life and everything else?\"? Answer\\n\\n### [Question 3: What is the answer to the question \"What is'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\n",
    "    \"What is the answer to the universe, life and everything else?\", \n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    max_length=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05471a29-a6c8-497b-b831-08e09952c445",
   "metadata": {},
   "source": [
    "## Sidenote\n",
    "\n",
    "At least for macOS, the MPS device is not necessarily better, even for 1000-rank matrices. \n",
    "See https://github.com/pytorch/pytorch/issues/77799:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f28bced-cd85-4c7a-81a5-35007020fd06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:01:17.356647Z",
     "iopub.status.busy": "2023-11-22T17:01:17.355982Z",
     "iopub.status.idle": "2023-11-22T17:03:19.585802Z",
     "shell.execute_reply": "2023-11-22T17:03:19.585153Z",
     "shell.execute_reply.started": "2023-11-22T17:01:17.356622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 2.0286582079716027\n",
      "mps 4.238669167039916\n",
      "More parallel\n",
      "cpu 9.15772208396811\n",
      "mps 4.447314416989684\n",
      "Even bigger!\n",
      "cpu 85.16490350000095\n",
      "mps 16.852848957991228\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "a_cpu = torch.rand(1000, device='cpu')\n",
    "b_cpu = torch.rand((1000, 1000), device='cpu')\n",
    "a_mps = torch.rand(1000, device='mps')\n",
    "b_mps = torch.rand((1000, 1000), device='mps')\n",
    " \n",
    "print('cpu', timeit.timeit(lambda: a_cpu @ b_cpu, number=100_000))\n",
    "print('mps', timeit.timeit(lambda: a_mps @ b_mps, number=100_000))\n",
    "\n",
    "print(\"More parallel\")\n",
    "print('cpu', timeit.timeit(lambda: b_cpu @ b_cpu, number=10_000))\n",
    "print('mps', timeit.timeit(lambda: b_mps @ b_mps, number=10_000))\n",
    "\n",
    "print(\"Even bigger!\")\n",
    "b_cpu = torch.rand((10000, 10000), device='cpu')\n",
    "b_mps = torch.rand((10000, 10000), device='mps')\n",
    "\n",
    "print('cpu', timeit.timeit(lambda: b_cpu @ b_cpu, number=100))\n",
    "print('mps', timeit.timeit(lambda: b_mps @ b_mps, number=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
